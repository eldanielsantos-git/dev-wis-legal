# 10 - Fluxo Completo de AnÃ¡lise

## ğŸ“‹ VisÃ£o Geral

Este documento detalha o fluxo end-to-end de processamento e anÃ¡lise de um documento jurÃ­dico no WisLegal, desde o upload inicial atÃ© a anÃ¡lise forense completa.

## ğŸ”„ Diagrama de Fluxo Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. User Upload  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Frontend Validation      â”‚
â”‚  - Formato (PDF only)        â”‚
â”‚  - Tamanho mÃ¡ximo            â”‚
â”‚  - Contagem de pÃ¡ginas       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Create Processo Record   â”‚
â”‚  - Status: 'uploading'       â”‚
â”‚  - Gerar UUID                â”‚
â”‚  - Associar user_id          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚                  â”‚
         â†“               â†“                  â†“
    Pequeno         MÃ©dio               Grande
    (<50MB)        (50-500MB)         (>500MB)
         â”‚               â”‚                  â”‚
         â†“               â†“                  â†“
   Upload Direto   Upload Direto      Chunking
   para GCS        para GCS           Inteligente
         â”‚               â”‚                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Storage & Database               â”‚
â”‚  - Upload para Google Cloud Storage  â”‚
â”‚  - Converter para Base64             â”‚
â”‚  - Armazenar no PostgreSQL           â”‚
â”‚  - Status: 'created'                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Edge Function: start-analysis    â”‚
â”‚  - Validar processo                  â”‚
â”‚  - Status: 'analyzing'               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Upload para Gemini File API      â”‚
â”‚  - POST para /upload/v1beta/files    â”‚
â”‚  - Aguardar state = 'ACTIVE'         â”‚
â”‚  - Salvar file_uri no banco          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. Loop: process-next-prompt        â”‚
â”‚  (Executado 9 vezes sequencialmente) â”‚
â”‚                                       â”‚
â”‚  Para cada prompt:                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 7.1. Buscar prÃ³ximo prompt     â”‚  â”‚
â”‚  â”‚ 7.2. Carregar modelo ativo     â”‚  â”‚
â”‚  â”‚ 7.3. Chamar Gemini API         â”‚  â”‚
â”‚  â”‚ 7.4. Parsing JSON (5 tentativas)â”‚ â”‚
â”‚  â”‚ 7.5. Salvar resultado          â”‚  â”‚
â”‚  â”‚ 7.6. Atualizar progresso       â”‚  â”‚
â”‚  â”‚ 7.7. Debitar tokens            â”‚  â”‚
â”‚  â”‚ 7.8. Criar notificaÃ§Ã£o         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. FinalizaÃ§Ã£o                      â”‚
â”‚  - Status: 'completed'               â”‚
â”‚  - analysis_completed_at = NOW()     â”‚
â”‚  - NotificaÃ§Ã£o ao usuÃ¡rio            â”‚
â”‚  - AnÃ¡lise disponÃ­vel                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Detalhamento por Etapa

### Etapa 1: User Upload

**Componente:** `FileUpload.tsx`

**AÃ§Ãµes do UsuÃ¡rio:**
1. Clica em "Novo Processo" ou Ã¡rea de upload
2. Seleciona arquivo PDF (ou drag & drop)
3. Confirma upload

**ValidaÃ§Ãµes Frontend:**
```typescript
// Formato
if (!file.type.includes('pdf')) {
  throw new Error('Apenas arquivos PDF sÃ£o aceitos');
}

// Tamanho (3GB mÃ¡ximo)
if (file.size > 3 * 1024 * 1024 * 1024) {
  throw new Error('Arquivo muito grande (mÃ¡x 3GB)');
}

// Contagem de pÃ¡ginas
const pageCount = await countPdfPages(file);
if (pageCount === 0) {
  throw new Error('PDF invÃ¡lido ou vazio');
}
```

### Etapa 2: Frontend Validation

**ServiÃ§o:** `ProcessosService.ts`

**MÃ©todo:** `countPdfPages(file: File)`

```typescript
const arrayBuffer = await file.arrayBuffer();
const pdf = await pdfjsLib.getDocument({ data: arrayBuffer }).promise;
return pdf.numPages;
```

**DecisÃ£o de EstratÃ©gia:**
```typescript
if (pageCount <= 50) {
  strategy = 'direct_upload';
} else if (pageCount <= 1000) {
  strategy = 'standard_upload';
} else {
  strategy = 'chunked_upload';
  chunksCount = Math.ceil(pageCount / 1000);
}
```

### Etapa 3: Create Processo Record

**MÃ©todo:** `ProcessosService.uploadAndStartProcessing()`

**SQL Executado:**
```sql
INSERT INTO processos (
  id,
  user_id,
  file_name,
  file_size,
  status,
  transcricao
) VALUES (
  $1,  -- UUID gerado
  $2,  -- user.id do auth
  $3,  -- file.name
  $4,  -- file.size
  'uploading',
  jsonb_build_object('totalPages', $5)
);
```

### Etapa 4: Storage & Database

**Upload para GCS:**
```typescript
const fileName = `${user.id}/${Date.now()}-${sanitizedFileName}`;

const { data, error } = await supabase.storage
  .from('processos')
  .upload(fileName, file, {
    cacheControl: '3600',
    upsert: false
  });
```

**ConversÃ£o Base64:**
```typescript
const reader = new FileReader();
reader.readAsDataURL(file);
const base64 = reader.result.split(',')[1];
```

**EstratÃ©gia de Armazenamento:**

#### Arquivos Pequenos (<50MB)
```sql
UPDATE processos
SET
  pdf_base64 = $1,
  is_chunked = FALSE
WHERE id = $2;
```

#### Arquivos Grandes (>50MB)
```sql
-- Divide em chunks de 40MB
INSERT INTO pdf_chunks (processo_id, chunk_number, chunk_data)
VALUES
  ($1, 1, $2),
  ($1, 2, $3),
  ...;

UPDATE processos
SET
  is_chunked = TRUE,
  total_chunks = $N
WHERE id = $1;
```

### Etapa 5: Edge Function - start-analysis

**Arquivo:** `supabase/functions/start-analysis/index.ts`

**ValidaÃ§Ãµes:**
```typescript
// 1. Processo existe?
const { data: processo } = await supabase
  .from('processos')
  .select('*')
  .eq('id', processo_id)
  .single();

if (!processo) {
  throw new Error('Processo nÃ£o encontrado');
}

// 2. Status correto?
if (processo.status !== 'created') {
  throw new Error('Processo jÃ¡ estÃ¡ sendo processado');
}

// 3. UsuÃ¡rio tem tokens?
const hasTokens = await checkTokenAvailability(
  processo.user_id,
  ESTIMATED_TOKENS
);

if (!hasTokens) {
  throw new Error('Tokens insuficientes');
}
```

**AtualizaÃ§Ã£o de Status:**
```sql
UPDATE processos
SET
  status = 'analyzing',
  analysis_started_at = NOW(),
  total_prompts = 9
WHERE id = $1;
```

### Etapa 6: Upload para Gemini File API

**Arquivo:** `supabase/functions/upload-to-gemini/index.ts`

**Processo:**
```typescript
// 1. Recuperar PDF
let pdfBuffer: ArrayBuffer;

if (processo.is_chunked) {
  // Reconstituir de chunks
  const { data: chunks } = await supabase
    .from('pdf_chunks')
    .select('chunk_data')
    .eq('processo_id', processo_id)
    .order('chunk_number');

  const base64 = chunks.map(c => c.chunk_data).join('');
  pdfBuffer = Buffer.from(base64, 'base64');
} else {
  // Direto do campo
  pdfBuffer = Buffer.from(processo.pdf_base64, 'base64');
}

// 2. Upload para Gemini
const formData = new FormData();
formData.append('file', new Blob([pdfBuffer]), 'processo.pdf');

const response = await fetch(
  `https://generativelanguage.googleapis.com/upload/v1beta/files?key=${apiKey}`,
  { method: 'POST', body: formData }
);

const { file } = await response.json();

// 3. Polling atÃ© ACTIVE
while (file.state === 'PROCESSING') {
  await new Promise(r => setTimeout(r, 2000));

  const statusResponse = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/${file.name}?key=${apiKey}`
  );
  const status = await statusResponse.json();

  if (status.state === 'ACTIVE') break;
  if (status.state === 'FAILED') throw new Error('Upload falhou');
}

// 4. Salvar URI
await supabase
  .from('processos')
  .update({
    gemini_file_uri: file.uri,
    gemini_file_name: file.name
  })
  .eq('id', processo_id);
```

### Etapa 7: Loop - process-next-prompt

**Arquivo:** `supabase/functions/process-next-prompt/index.ts`

**Executado 9 vezes sequencialmente** (um prompt por vez)

#### 7.1. Buscar PrÃ³ximo Prompt

```sql
SELECT *
FROM analysis_prompts
WHERE is_active = TRUE
  AND execution_order = (
    SELECT current_prompt_number + 1
    FROM processos
    WHERE id = $1
  )
ORDER BY execution_order
LIMIT 1;
```

#### 7.2. Carregar Modelo Ativo

```sql
SELECT *
FROM admin_system_models
WHERE is_active = TRUE
ORDER BY priority DESC
LIMIT 1;
```

#### 7.3. Chamar Gemini API

```typescript
const response = await fetch(
  `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent?key=${apiKey}`,
  {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      contents: [{
        role: 'user',
        parts: [
          { fileData: { fileUri: gemini_file_uri } },
          { text: prompt.content }
        ]
      }],
      generationConfig: {
        temperature: 0.2,
        topP: 0.8,
        topK: 40,
        maxOutputTokens: 8192,
        responseMimeType: 'application/json'
      }
    })
  }
);

const result = await response.json();
const textResponse = result.candidates[0].content.parts[0].text;
```

#### 7.4. Parsing JSON (5 EstratÃ©gias)

```typescript
function parseJSONResponse(text: string): any {
  // EstratÃ©gia 1: JSON puro
  try {
    return JSON.parse(text);
  } catch {}

  // EstratÃ©gia 2: Remove markdown
  try {
    const cleaned = text.replace(/```json\n?|\n?```/g, '');
    return JSON.parse(cleaned);
  } catch {}

  // EstratÃ©gia 3: Extrai primeiro objeto
  try {
    const match = text.match(/\{[\s\S]*\}/);
    return JSON.parse(match[0]);
  } catch {}

  // EstratÃ©gia 4: Extrai atÃ© citations_index
  try {
    const match = text.match(/\{[\s\S]*"citations_index"[\s\S]*?\]/);
    return JSON.parse(match[0] + '}');
  } catch {}

  // EstratÃ©gia 5: Fallback
  return {
    error: 'Parsing failed',
    raw_response: text.substring(0, 500)
  };
}
```

#### 7.5. Salvar Resultado

```sql
INSERT INTO analysis_results (
  processo_id,
  prompt_id,
  execution_order,
  result,
  status,
  model_name,
  tokens_used,
  execution_time_ms
) VALUES ($1, $2, $3, $4, 'completed', $5, $6, $7);

-- TambÃ©m atualiza campo legacy no processos
UPDATE processos
SET
  visao_geral_processo = $result  -- se execution_order = 1
WHERE id = $processo_id;
```

#### 7.6. Atualizar Progresso

```sql
UPDATE processos
SET
  current_prompt_number = current_prompt_number + 1,
  updated_at = NOW()
WHERE id = $1;
```

#### 7.7. Debitar Tokens

```sql
-- Atualiza subscription
UPDATE stripe_subscriptions
SET tokens_used = tokens_used + $tokens
WHERE customer_id = (
  SELECT customer_id FROM stripe_customers
  WHERE user_id = $user_id
);

-- Log de auditoria
INSERT INTO token_usage_logs (
  user_id,
  processo_id,
  operation_type,
  tokens_used,
  model_name
) VALUES ($1, $2, 'analysis', $3, $4);
```

#### 7.8. NotificaÃ§Ã£o (se Ãºltimo prompt)

```sql
-- Se current_prompt_number = total_prompts
INSERT INTO notifications (
  user_id,
  type,
  message,
  processo_id
) VALUES (
  $user_id,
  'success',
  'AnÃ¡lise concluÃ­da: ' || $file_name,
  $processo_id
);
```

### Etapa 8: FinalizaÃ§Ã£o

**Quando:** `current_prompt_number = total_prompts`

**SQL:**
```sql
UPDATE processos
SET
  status = 'completed',
  analysis_completed_at = NOW()
WHERE id = $1;
```

**NotificaÃ§Ã£o Push:**
```typescript
// Via Supabase Realtime
// Frontend recebe automaticamente via subscription
```

**Som de NotificaÃ§Ã£o:**
```typescript
// Frontend: notificationSound.ts
const audio = new Audio('/notification.mp3');
audio.play();
```

## â±ï¸ Tempo de Processamento

### Por Tamanho de Documento

| PÃ¡ginas | Tier | Upload | OCR | AnÃ¡lise IA | Total |
|---------|------|--------|-----|------------|-------|
| 1-50 | T1 | 5s | - | 2min | ~2min |
| 51-200 | T2 | 10s | - | 3min | ~3min |
| 201-500 | T3 | 20s | - | 4min | ~4.5min |
| 501-1000 | T4 | 40s | - | 5min | ~6min |
| 1001-5000 | T5 | 2min | - | 10min | ~12min |

## ğŸ”„ Fluxo de Retry

### Em Caso de Falha

```typescript
// process-next-prompt com retry
for (let attempt = 1; attempt <= 3; attempt++) {
  try {
    await processPrompt();
    break; // Sucesso
  } catch (error) {
    if (attempt === 3) {
      // Marca como erro apÃ³s 3 tentativas
      await markAsError(error);
    } else {
      // Aguarda antes de retentar (backoff exponencial)
      await sleep(1000 * attempt);
    }
  }
}
```

## ğŸ”— PrÃ³ximos Documentos

- **[11-SISTEMA-PROMPTS.md](./11-SISTEMA-PROMPTS.md)** - Prompts de IA
- **[06-INTEGRACOES-GCP.md](./06-INTEGRACOES-GCP.md)** - Google Cloud

---

**Fluxo completo: Upload â†’ AnÃ¡lise â†’ Insights**
