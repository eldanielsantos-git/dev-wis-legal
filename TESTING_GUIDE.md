# Guia de Teste - OtimizaÃ§Ãµes de Processamento

## Como Testar as OtimizaÃ§Ãµes

### 1. PreparaÃ§Ã£o

Certifique-se de que:
- âœ… A migration foi aplicada no banco de dados Supabase
- âœ… O build do projeto foi feito com sucesso (`npm run build`)
- âœ… As edge functions foram deployadas (se necessÃ¡rio)

### 2. Teste com Documento de 3000 PÃ¡ginas

#### 2.1. Upload do Documento

1. FaÃ§a login no sistema
2. VÃ¡ para a pÃ¡gina de upload
3. Selecione um PDF com aproximadamente 3000 pÃ¡ginas
4. Inicie o upload

#### 2.2. Monitoramento no Console do Navegador

Abra o DevTools (F12) e monitore os logs. VocÃª deverÃ¡ ver:

```
â±ï¸ Tempo estimado: ~10-15 minutos
ğŸ“¦ Documento detectado com 3000 pÃ¡ginas
âœ… Dividindo em 5 chunks de 600 pÃ¡ginas
ğŸš€ Iniciando processamento complexo...
```

#### 2.3. Monitoramento no Banco de Dados

Execute estas queries no Supabase SQL Editor:

**Verificar status de processamento:**
```sql
SELECT
  processo_id,
  current_phase,
  current_active_workers,
  max_concurrent_workers,
  chunks_completed,
  total_chunks,
  overall_progress_percent,
  avg_chunk_processing_seconds
FROM complex_processing_status
WHERE processo_id = 'SEU_PROCESSO_ID'  -- Substituir pelo ID do processo
ORDER BY created_at DESC
LIMIT 1;
```

**Verificar workers ativos:**
```sql
SELECT
  processo_id,
  active_worker_ids,
  jsonb_array_length(active_worker_ids) as workers_count,
  last_heartbeat
FROM complex_processing_status
WHERE processo_id = 'SEU_PROCESSO_ID'
AND current_active_workers > 0;
```

**Verificar progresso da fila:**
```sql
SELECT
  status,
  COUNT(*) as count,
  AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_time_seconds
FROM processing_queue
WHERE processo_id = 'SEU_PROCESSO_ID'
GROUP BY status;
```

#### 2.4. Monitoramento nos Logs do Supabase

1. VÃ¡ para Supabase Dashboard â†’ Edge Functions
2. Abra os logs de `process-complex-worker`
3. Procure por mÃºltiplos workers rodando simultaneamente:

```
[abc123] ğŸ”„ Worker iniciado
[def456] ğŸ”„ Worker iniciado
[ghi789] ğŸ”„ Worker iniciado
[jkl012] ğŸ”„ Worker iniciado
[mno345] ğŸ”„ Worker iniciado
```

### 3. VerificaÃ§Ã£o de Sucesso

#### 3.1. Tempo de Processamento

âœ… **Sucesso**: Documento de 3000 pÃ¡ginas processa em **10-15 minutos**
âš ï¸ **Alerta**: Se levar mais de 20 minutos, verificar logs para erros
âŒ **Falha**: Se levar mais de 30 minutos, algo estÃ¡ errado

#### 3.2. Workers Paralelos

Execute durante o processamento:
```sql
SELECT
  current_active_workers,
  max_concurrent_workers
FROM complex_processing_status
WHERE processo_id = 'SEU_PROCESSO_ID';
```

âœ… **Sucesso**: `current_active_workers` deve estar entre 3-5 durante processamento
âš ï¸ **Alerta**: Se sempre 1, paralelizaÃ§Ã£o nÃ£o estÃ¡ funcionando
âŒ **Falha**: Se 0, processamento nÃ£o iniciou

#### 3.3. Chunks Processados

```sql
SELECT
  total_chunks,
  chunks_completed,
  chunks_processing,
  chunks_pending
FROM complex_processing_status
WHERE processo_id = 'SEU_PROCESSO_ID';
```

âœ… **Sucesso**: `total_chunks` deve ser ~5 (para 3000 pÃ¡ginas)
âŒ **Falha**: Se `total_chunks` > 10, chunks ainda estÃ£o com 300 pÃ¡ginas

### 4. Teste de MÃ©tricas

ApÃ³s completar o processamento:

```sql
SELECT
  avg_chunk_processing_seconds,
  total_chunks_completed,
  (chunks_completed::float / total_chunks * 100) as completion_percent
FROM complex_processing_status
WHERE processo_id = 'SEU_PROCESSO_ID';
```

âœ… **Esperado**:
- `avg_chunk_processing_seconds`: 240-360 segundos (4-6 min)
- `total_chunks_completed`: 5
- `completion_percent`: 100

### 5. ComparaÃ§Ã£o Antes/Depois

| MÃ©trica | Antes (300p/chunk) | Depois (600p/chunk + 5 workers) |
|---------|-------------------|----------------------------------|
| Total de chunks | 10 | 5 |
| Total de tarefas | 90 | 45 |
| Workers paralelos | 1 | 5 |
| Tempo estimado | 120 min | 10-15 min |
| Tempo mÃ©dio/chunk | 2-3 min | 4-6 min |

### 6. Troubleshooting

#### Problema: Workers nÃ£o estÃ£o rodando em paralelo

**Verificar**:
```sql
SELECT max_concurrent_workers, current_active_workers
FROM complex_processing_status
WHERE processo_id = 'SEU_PROCESSO_ID';
```

**SoluÃ§Ã£o**:
- Se `max_concurrent_workers = 0`: A migration nÃ£o foi aplicada
- Se `current_active_workers = 0`: Nenhum worker foi disparado

#### Problema: Processamento muito lento

**Verificar logs** de `process-complex-worker`:
- Procurar por erros de API Gemini
- Verificar se hÃ¡ rate limiting (429 errors)
- Confirmar que chunks estÃ£o ACTIVE no Gemini

**SoluÃ§Ã£o**:
- Reduzir workers: `UPDATE complex_processing_status SET max_concurrent_workers = 3`
- Verificar configuraÃ§Ã£o da API Gemini

#### Problema: Chunks ainda com 300 pÃ¡ginas

**Verificar** no cÃ³digo:
```typescript
// src/utils/pdfSplitter.ts
const CHUNK_SIZE_NORMAL = 600; // Deve ser 600, nÃ£o 300
```

**SoluÃ§Ã£o**:
- Rebuild: `npm run build`
- Clear cache do navegador
- Fazer novo upload

### 7. Teste de Carga (Opcional)

Para testar com mÃºltiplos documentos:

1. Upload de 2-3 documentos de 3000 pÃ¡ginas simultaneamente
2. Verificar distribuiÃ§Ã£o de workers:

```sql
SELECT
  processo_id,
  current_active_workers,
  max_concurrent_workers,
  current_phase
FROM complex_processing_status
WHERE current_phase IN ('processing', 'consolidating')
ORDER BY created_at DESC;
```

âœ… **Esperado**: Workers distribuÃ­dos entre processos
âš ï¸ **Alerta**: Se um processo monopoliza todos os workers

### 8. ValidaÃ§Ã£o Final

ApÃ³s o processamento completar:

1. âœ… Verificar que todos os 9 prompts foram processados
2. âœ… Verificar que anÃ¡lise estÃ¡ completa e visÃ­vel no frontend
3. âœ… Verificar que nÃ£o hÃ¡ chunks travados na fila
4. âœ… Verificar que workers foram desregistrados

```sql
-- Verificar anÃ¡lises completas
SELECT COUNT(*) as completed_analyses
FROM analysis_results
WHERE processo_id = 'SEU_PROCESSO_ID'
AND status = 'completed';
-- Deve retornar 9

-- Verificar se nÃ£o hÃ¡ tarefas travadas
SELECT status, COUNT(*)
FROM processing_queue
WHERE processo_id = 'SEU_PROCESSO_ID'
GROUP BY status;
-- Todas devem estar 'completed'

-- Verificar workers desregistrados
SELECT current_active_workers
FROM complex_processing_status
WHERE processo_id = 'SEU_PROCESSO_ID';
-- Deve retornar 0
```

## Resultado Esperado

Para um documento de **3000 pÃ¡ginas**:

- â±ï¸ **Tempo**: 10-15 minutos (vs 120 minutos antes)
- ğŸ“Š **Chunks**: 5 (vs 10 antes)
- ğŸ‘· **Workers**: 5 paralelos (vs 1 antes)
- âœ… **Taxa de sucesso**: >95%
- ğŸš€ **Ganho**: ~90% mais rÃ¡pido

## Ajustes Recomendados

Se apÃ³s o teste vocÃª observar:

**1. Processamento muito rÃ¡pido (<8 minutos)**
- Aumentar workers para 8:
```sql
UPDATE complex_processing_status
SET max_concurrent_workers = 8
WHERE processo_id = 'SEU_PROCESSO_ID';
```

**2. Rate limiting da API Gemini**
- Reduzir workers para 3:
```sql
UPDATE complex_processing_status
SET max_concurrent_workers = 3
WHERE processo_id = 'SEU_PROCESSO_ID';
```

**3. Uso alto de memÃ³ria**
- Reduzir tamanho dos chunks ou workers

## PrÃ³ximos Passos

ApÃ³s validar o teste:
1. Monitorar por 1-2 dias com documentos reais
2. Ajustar `max_concurrent_workers` conforme necessÃ¡rio
3. Implementar dashboard visual (opcional)
4. Considerar auto-scaling baseado em carga (opcional)
